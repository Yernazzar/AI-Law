import os
import json
import hashlib
import shutil
from typing import List, Optional
from datetime import datetime
from pydantic import BaseModel
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_community.vectorstores import FAISS as LC_FAISS
from langchain.docstore.document import Document
from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain
from langchain.prompts.prompt import PromptTemplate
from langchain.embeddings.base import Embeddings
import docx
import pdfplumber
import chardet
from razdel import sentenize
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import warnings
import telebot
import logging
from simulation import SimulationManager, SimulationState

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger(__name__)

# Load environment variables
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    warnings.warn("OPENAI_API_KEY not found in .env file")

# Telegram Token
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_KEY")

# Folders setup
DATA_FOLDER = os.getenv("DATA_FOLDER", "legal_data")
INDEXES_FOLDER = os.getenv("INDEXES_FOLDER", "legal_indexes")
os.makedirs(DATA_FOLDER, exist_ok=True)
os.makedirs(INDEXES_FOLDER, exist_ok=True)


# Embeddings class for text
class MyEmbeddings(Embeddings):
    def __init__(self):
        self.model = SentenceTransformer('all-MiniLM-L12-v2')

    def embed_documents(self, texts):
        return self.model.encode(texts, convert_to_numpy=True).tolist()

    def embed_query(self, text):
        return self.model.encode([text], convert_to_numpy=True)[0].tolist()


# Document manager class
class DocumentManager:
    def __init__(self, data_folder, metadata_file="legal_metadata.json"):
        self.data_folder = data_folder
        self.metadata_file = os.path.join(data_folder, metadata_file)
        self.ensure_metadata_exists()

    def ensure_metadata_exists(self):
        os.makedirs(self.data_folder, exist_ok=True)
        if not os.path.exists(self.metadata_file):
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump({"documents": []}, f, ensure_ascii=False, indent=2)

    def load_metadata(self):
        with open(self.metadata_file, 'r', encoding='utf-8') as f:
            return json.load(f)

    def save_metadata(self, metadata):
        with open(self.metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

    def calculate_hash(self, file_path):
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def extract_full_text(self, file_path):
        file_ext = os.path.splitext(file_path)[1].lower()
        try:
            if file_ext == '.docx':
                doc = docx.Document(file_path)
                return "\n".join(p.text for p in doc.paragraphs if p.text.strip())
            elif file_ext == '.pdf':
                with pdfplumber.open(file_path) as pdf:
                    return "\n".join(page.extract_text() or "" for page in pdf.pages)
            elif file_ext == '.txt':
                with open(file_path, 'rb') as f:
                    raw_data = f.read()
                    detected_encoding = chardet.detect(raw_data)['encoding']
                with open(file_path, 'r', encoding=detected_encoding or 'utf-8', errors='replace') as file:
                    return file.read()
            return ""
        except Exception as e:
            logger.error(f"Error extracting text from {file_path}: {str(e)}")
            return ""

    def get_active_documents(self):
        metadata = self.load_metadata()
        return [doc for doc in metadata.get("documents", []) if doc.get("status") == "active"]


# Document processing functions
def split_text_into_sentences(text):
    return [s.text for s in sentenize(text)]


def create_chunks_by_sentence(text, file_metadata, target_chunk_size=512, min_chunk_size=256, overlap_size=50):
    sentences = split_text_into_sentences(text)
    chunks = []
    current_chunk = []
    current_size = 0

    for sentence in sentences:
        sentence_size = len(sentence.split())
        if current_size + sentence_size > target_chunk_size and current_chunk:
            chunk_text = " ".join(current_chunk)
            chunks.append(chunk_text)
            if overlap_size > 0:
                overlap_sentences = []
                overlap_tokens = 0
                j = len(current_chunk) - 1
                while j >= 0 and overlap_tokens < overlap_size:
                    overlap_sentences.insert(0, current_chunk[j])
                    overlap_tokens += len(current_chunk[j].split())
                    j -= 1
                current_chunk = overlap_sentences + [sentence]
                current_size = sum(len(s.split()) for s in current_chunk)
            else:
                current_chunk = [sentence]
                current_size = sentence_size
        else:
            current_chunk.append(sentence)
            current_size += sentence_size

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    chunk_dicts = []
    for idx, chunk in enumerate(chunks):
        if len(chunk.split()) >= min_chunk_size:
            data = {
                "id": f"{file_metadata.get('file_name', 'unknown')}-chunk-{idx}",
                "text": chunk,
                "metadata": {**file_metadata, "chunk_id": idx, "token_count": len(chunk.split())}
            }
            chunk_dicts.append(data)
    return chunk_dicts


def process_document(file_path, target_chunk_size=512, min_chunk_size=256, overlap_size=50):
    file_extension = os.path.splitext(file_path)[1].lower()
    file_metadata = {"file_name": os.path.basename(file_path), "file_path": file_path}
    text = ""

    if file_extension == '.docx':
        doc = docx.Document(file_path)
        text = "\n".join(p.text for p in doc.paragraphs if p.text.strip())
        file_metadata["file_type"] = "docx"
    elif file_extension == '.pdf':
        with pdfplumber.open(file_path) as pdf:
            text = "\n".join(page.extract_text() or "" for page in pdf.pages)
            file_metadata["file_type"] = "pdf"
            file_metadata["num_pages"] = len(pdf.pages)
    elif file_extension == '.txt':
        with open(file_path, 'rb') as f:
            raw_data = f.read()
            detected_encoding = chardet.detect(raw_data)['encoding']
        with open(file_path, 'r', encoding=detected_encoding or 'utf-8', errors='replace') as file:
            text = file.read()
        file_metadata["file_type"] = "txt"
    else:
        logger.info(f"Unsupported file type: {file_path}")
        return []

    if len(text.strip()) < 50:
        logger.info(f"File {file_metadata['file_name']} contains less than 50 characters.")
        return []
    return create_chunks_by_sentence(text, file_metadata, target_chunk_size, min_chunk_size, overlap_size)


def process_document_folder(folder_path, **kwargs):
    all_chunks = []
    for root, _, files in os.walk(folder_path):
        for file in files:
            if file.lower().endswith(('.docx', '.pdf', '.txt')) and not file.startswith('.'):
                file_path = os.path.join(root, file)
                if file != "legal_metadata.json":
                    logger.debug(f"Processing file: {file_path}")
                    chunks = process_document(file_path, **kwargs)
                    all_chunks.extend(chunks)
    return all_chunks


# Vector storage functions
def create_faiss_index(chunks, embeddings_model):
    texts = [chunk["text"] for chunk in chunks]
    if not texts:
        vector_size = embeddings_model.embed_query("Empty index").shape[0]
        index = faiss.IndexFlatL2(vector_size)
        return index, []

    embed_array = np.array(embeddings_model.embed_documents(texts)).astype("float32")
    vector_size = embed_array.shape[1]
    index = faiss.IndexFlatL2(vector_size)
    index.add(embed_array)
    return index, embed_array


def save_faiss_index(index, metadata, index_path, metadata_path):
    faiss.write_index(index, index_path)
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)


def load_or_rebuild_vectorstore(data_folder, indexes_folder, embeddings_model):
    os.makedirs(indexes_folder, exist_ok=True)
    fingerprint_file = os.path.join(indexes_folder, "index_fingerprint.json")
    index_path = os.path.join(indexes_folder, "index.faiss")
    metadata_path = os.path.join(indexes_folder, "document_metadata.json")

    current_fingerprint = {}
    for root, _, files in os.walk(data_folder):
        for file in files:
            if file.lower().endswith(('.docx', '.pdf', '.txt')) and not file.startswith('.'):
                if file != "legal_metadata.json":
                    path = os.path.join(root, file)
                    current_fingerprint[path] = os.path.getmtime(path)

    fingerprint_hash = hashlib.md5(json.dumps(current_fingerprint, sort_keys=True).encode()).hexdigest()

    previous_fingerprint = None
    if os.path.exists(fingerprint_file):
        with open(fingerprint_file, 'r') as f:
            try:
                previous_fingerprint = json.load(f)
            except json.JSONDecodeError:
                previous_fingerprint = None

    if (os.path.exists(index_path) and os.path.exists(metadata_path) and
            previous_fingerprint == fingerprint_hash):
        try:
            logger.info("Loading existing vector store...")
            vectorstore = LC_FAISS.load_local(indexes_folder, embeddings_model, allow_dangerous_deserialization=True)
            return vectorstore
        except Exception as e:
            logger.error(f"Failed to load vector store: {e}")
            if os.path.exists(index_path):
                os.remove(index_path)
            if os.path.exists(metadata_path):
                os.remove(metadata_path)

    logger.info("Recreating vector index...")
    chunks = process_document_folder(data_folder, target_chunk_size=512, min_chunk_size=256, overlap_size=150)

    if not chunks:
        logger.info("No documents found for indexing. Creating empty index.")
        vectorstore = LC_FAISS.from_documents(
            [Document(page_content="Empty index", metadata={"source": "empty"})],
            embeddings_model
        )
        vectorstore.save_local(indexes_folder)
        with open(fingerprint_file, 'w') as f:
            json.dump(fingerprint_hash, f)
        return vectorstore

    docs = [Document(page_content=ch["text"], metadata=ch["metadata"]) for ch in chunks]
    vectorstore = LC_FAISS.from_documents(docs, embeddings_model)
    vectorstore.save_local(indexes_folder)
    with open(fingerprint_file, 'w') as f:
        json.dump(fingerprint_hash, f)
    return vectorstore


# Prompt template
def get_legal_prompt_template():
    return (
        "Ð’Ñ‹ â€” Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð¿Ð¾ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸ÑÐ¼ Ð´Ð»Ñ Ð¶Ð¸Ñ‚ÐµÐ»ÐµÐ¹, Ð½Ð°Ð·Ñ‹Ð²Ð°ÐµÑ‚ÐµÑÑŒ LegalAidBot. "
        "Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ð¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð´Ð»Ñ Ñ‚Ð¾Ñ‡Ð½Ð¾Ð³Ð¾ Ð¸ ÐºÑ€Ð°Ñ‚ÐºÐ¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ. "
        "ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹Ñ‚Ðµ Ð½Ð° ÑÐ·Ñ‹ÐºÐµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°. "
        "Ð’Ð½Ð¸Ð¼Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹Ñ‚Ðµ Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¿ÐµÑ€ÐµÐ´ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð¼. "
        "Ð”Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ñ‡ÐµÑ‚ÐºÐ¸Ðµ, ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð¼Ð°Ñ€ÐºÐ¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð»Ð¸ Ð½ÑƒÐ¼ÐµÑ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¿Ð¸ÑÐºÐ¸, Ð³Ð´Ðµ ÑƒÐ¼ÐµÑÑ‚Ð½Ð¾. "
        "Ð‘ÑƒÐ´ÑŒÑ‚Ðµ Ð´Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹ Ð¸ Ð²Ñ‹ÑÑ‚ÑƒÐ¿Ð°Ð¹Ñ‚Ðµ Ð² Ñ€Ð¾Ð»Ð¸ Ð¿Ð¾Ð»ÐµÐ·Ð½Ð¾Ð³Ð¾ Ð³Ð¸Ð´Ð°. "
        "Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ñ‡Ð°Ñ‚Ð°:\n{chat_history}\n\n"
        "ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:\n{context}\n\n"
        "Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {question}\n\n"
        "ÐžÑ‚Ð²ÐµÑ‚:"
    )


# Chat management class
class ChatAssistant:
    def __init__(self, qa_chain):
        self.qa = qa_chain
        self.histories = {}

    def get_answer(self, user_query: str, session_id: str = "default"):
        if session_id not in self.histories:
            self.histories[session_id] = []

        chain_history = []
        for i in range(0, len(self.histories[session_id]), 2):
            if i + 1 < len(self.histories[session_id]):
                user_msg = self.histories[session_id][i]["content"]
                assistant_msg = self.histories[session_id][i + 1]["content"]
                chain_history.append((user_msg, assistant_msg))

        try:
            result = self.qa({"question": user_query, "chat_history": chain_history})
            answer = result.get("answer", "")
            if "Ð­Ñ‚Ð¾ Ð½Ðµ ÑÐ²Ð»ÑÐµÑ‚ÑÑ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸ÐµÐ¹" not in answer:
                answer += "\n\nÐ­Ñ‚Ð¾ Ð½Ðµ ÑÐ²Ð»ÑÐµÑ‚ÑÑ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÐºÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸ÐµÐ¹. ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¾Ð±Ñ€Ð°Ñ‚Ð¸Ñ‚ÐµÑÑŒ Ðº ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¼Ñƒ ÑŽÑ€Ð¸ÑÑ‚Ñƒ Ð´Ð»Ñ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð¸."
            source_docs = result.get("source_documents", [])
        except Exception as e:
            logger.error(f"Error processing request: {str(e)}")
            answer = "Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ Ð²Ð°ÑˆÐµÐ³Ð¾ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°. ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¿ÐµÑ€ÐµÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð²Ð¾Ð¿Ñ€Ð¾Ñ."
            source_docs = []

        self.histories[session_id].append({
            "role": "user",
            "content": user_query,
            "time": datetime.now().strftime("%H:%M:%S")
        })
        self.histories[session_id].append({
            "role": "assistant",
            "content": answer,
            "time": datetime.now().strftime("%H:%M:%S")
        })

        return answer, source_docs

    def clear_history(self, session_id: str = "default"):
        self.histories[session_id] = []


# Initialize components
logger.info("Initializing components...")
embeddings = MyEmbeddings()
doc_manager = DocumentManager(DATA_FOLDER)

try:
    logger.info("Setting up LLM and vector store...")
    llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, temperature=0, model_name="gpt-4o-mini")
    prompt = PromptTemplate(
        template=get_legal_prompt_template(),
        input_variables=["chat_history", "context", "question"]
    )
    vectorstore = load_or_rebuild_vectorstore(DATA_FOLDER, INDEXES_FOLDER, embeddings)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm,
        retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": prompt}
    )
    assistant = ChatAssistant(qa_chain)
    logger.info("Components initialized successfully.")
except Exception as e:
    logger.error(f"Error initializing components: {str(e)}")
    raise

# Initialize Telegram bot
simulation_manager = SimulationManager(qa_chain, vectorstore)
logger.info("Simulation manager initialized successfully.")
bot = telebot.TeleBot(TELEGRAM_BOT_TOKEN)


# Ð”Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ Ð½Ð¾Ð²Ñ‹Ðµ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ñ‡Ð¸ÐºÐ¸ ÐºÐ¾Ð¼Ð°Ð½Ð´
@bot.message_handler(commands=['simulation'])
def handle_simulation(message):
    user_id = str(message.from_user.id)

    # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ (Ñ‚ÐµÐ¼Ñƒ)
    command_args = message.text.split(' ', 1)
    topic = command_args[1] if len(command_args) > 1 else None

    # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ Ð½Ð°Ð±Ð¾Ñ€Ð° Ñ‚ÐµÐºÑÑ‚Ð°
    bot.send_chat_action(message.chat.id, 'typing')

    # ÐÐ°Ñ‡Ð¸Ð½Ð°ÐµÐ¼ ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸ÑŽ Ñ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð¹ Ñ‚ÐµÐ¼Ð¾Ð¹ Ð¸Ð»Ð¸ Ð±ÐµÐ· Ð½ÐµÑ‘
    response = simulation_manager.start_simulation(user_id, topic)

    # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚, Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°Ñ Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸ Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
    if len(response) > 4000:
        chunks = [response[i:i + 4000] for i in range(0, len(response), 4000)]
        for chunk in chunks:
            bot.send_message(message.chat.id, chunk, parse_mode='Markdown')
    else:
        bot.send_message(message.chat.id, response, parse_mode='Markdown')


@bot.message_handler(commands=['stop_simulation'])
def handle_stop_simulation(message):
    user_id = str(message.from_user.id)

    if simulation_manager.is_in_simulation(user_id):
        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ Ð½Ð°Ð±Ð¾Ñ€Ð° Ñ‚ÐµÐºÑÑ‚Ð°
        bot.send_chat_action(message.chat.id, 'typing')

        response = simulation_manager.end_simulation(user_id)

        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚, Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°Ñ Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸ Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
        if len(response) > 4000:
            chunks = [response[i:i + 4000] for i in range(0, len(response), 4000)]
            for chunk in chunks:
                bot.send_message(message.chat.id, chunk, parse_mode='Markdown')
        else:
            bot.send_message(message.chat.id, response, parse_mode='Markdown')
    else:
        bot.send_message(message.chat.id, "Ð£ Ð²Ð°Ñ Ð½ÐµÑ‚ Ð°ÐºÑ‚Ð¸Ð²Ð½Ð¾Ð¹ ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ð¸.")


# Define commands and handlers
@bot.message_handler(commands=['start'])
def handle_start(message):
    user_id = str(message.from_user.id)
    bot.send_message(
        message.chat.id,
        "ðŸ‘‹ Ð—Ð´Ñ€Ð°Ð²ÑÑ‚Ð²ÑƒÐ¹Ñ‚Ðµ! Ð¯ LegalAidBot - Ð²Ð°Ñˆ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚.\n\n"
        "Ð—Ð°Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ Ð¼Ð½Ðµ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð¸ Ñ Ð¿Ð¾ÑÑ‚Ð°Ñ€Ð°ÑŽÑÑŒ Ð½Ð° Ð½Ð¸Ñ… Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ.\n\n"
        "ÐšÐ¾Ð¼Ð°Ð½Ð´Ñ‹:\n"
        "/help - ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÑÐ¿Ñ€Ð°Ð²ÐºÑƒ\n"
        "/clear - ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð½Ð°ÑˆÐµÐ³Ð¾ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð°\n"
        "/simulation [Ñ‚ÐµÐ¼Ð°] - ÐÐ°Ñ‡Ð°Ñ‚ÑŒ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÑƒÑŽ ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸ÑŽ\n"
        "/stop_simulation - ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸ÑŽ"
    )
    assistant.clear_history(user_id)


@bot.message_handler(commands=['help'])
def handle_help(message):
    bot.send_message(
        message.chat.id,
        "Ð¯ - Ð²Ð°Ñˆ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚. Ð’Ð¾Ñ‚ Ñ‡Ñ‚Ð¾ Ñ Ð¼Ð¾Ð³Ñƒ:\n\n"
        "1. *ÐšÐ¾Ð½ÑÑƒÐ»ÑŒÑ‚Ð°Ñ†Ð¸Ñ:* Ð—Ð°Ð´Ð°Ð²Ð°Ð¹Ñ‚Ðµ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð¸ Ñ Ð¾Ñ‚Ð²ÐµÑ‡Ñƒ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹\n"
        "2. *Ð¡Ð¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ñ:* Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ /simulation [Ñ‚ÐµÐ¼Ð°], Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ð°Ñ‡Ð°Ñ‚ÑŒ ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸ÑŽ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÐ¾Ð¹ ÑÐ¸Ñ‚ÑƒÐ°Ñ†Ð¸Ð¸\n\n"
        "Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹:\n"
        "- /start - ÐÐ°Ñ‡Ð°Ñ‚ÑŒ Ð´Ð¸Ð°Ð»Ð¾Ð³\n"
        "- /help - ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÑÑ‚Ñƒ ÑÐ¿Ñ€Ð°Ð²ÐºÑƒ\n"
        "- /clear - ÐžÑ‡Ð¸ÑÑ‚Ð¸Ñ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð°\n"
        "- /simulation [Ñ‚ÐµÐ¼Ð°] - ÐÐ°Ñ‡Ð°Ñ‚ÑŒ ÑŽÑ€Ð¸Ð´Ð¸Ñ‡ÐµÑÐºÑƒÑŽ ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸ÑŽ\n"
        "- /stop_simulation - ÐžÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ Ñ‚ÐµÐºÑƒÑ‰ÑƒÑŽ ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸ÑŽ",
        parse_mode='Markdown'
    )


@bot.message_handler(commands=['clear'])
def handle_clear(message):
    user_id = str(message.from_user.id)
    assistant.clear_history(user_id)
    bot.send_message(
        message.chat.id,
        "ðŸ—‘ï¸ Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð° Ð¾Ñ‡Ð¸Ñ‰ÐµÐ½Ð°. Ð’Ñ‹ Ð¼Ð¾Ð¶ÐµÑ‚Ðµ Ð·Ð°Ð´Ð°Ñ‚ÑŒ Ð½Ð¾Ð²Ñ‹Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ."
    )


@bot.message_handler(func=lambda message: True)
def handle_message(message):
    user_id = str(message.from_user.id)
    user_query = message.text

    # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð¸Ð½Ð´Ð¸ÐºÐ°Ñ‚Ð¾Ñ€ Ð½Ð°Ð±Ð¾Ñ€Ð° Ñ‚ÐµÐºÑÑ‚Ð°
    bot.send_chat_action(message.chat.id, 'typing')

    # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼, Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ Ð»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð² ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ð¸
    if simulation_manager.is_in_simulation(user_id):
        # Ð•ÑÐ»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð² Ñ€ÐµÐ¶Ð¸Ð¼Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ð¸
        sim_state = simulation_manager.get_simulation_state(user_id)

        if sim_state == SimulationState.SETUP:
            # ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°ÐµÑ‚ Ñ‚ÐµÐ¼Ñƒ Ð´Ð»Ñ ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ð¸
            response = simulation_manager.start_simulation(user_id, user_query)
        elif sim_state == SimulationState.RUNNING:
            # ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÐµÑ‚ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð² ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸Ð¸
            response = simulation_manager.process_answer(user_id, user_query)
        else:
            # ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ðµ, Ð·Ð°ÐºÐ°Ð½Ñ‡Ð¸Ð²Ð°ÐµÐ¼ ÑÐ¸Ð¼ÑƒÐ»ÑÑ†Ð¸ÑŽ
            response = simulation_manager.end_simulation(user_id)

        # ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð¾Ñ‚Ð²ÐµÑ‚, Ñ€Ð°Ð·Ð±Ð¸Ð²Ð°Ñ Ð½Ð° Ñ‡Ð°ÑÑ‚Ð¸ Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
        if len(response) > 4000:
            chunks = [response[i:i + 4000] for i in range(0, len(response), 4000)]
            for chunk in chunks:
                bot.send_message(message.chat.id, chunk, parse_mode='Markdown')
        else:
            bot.send_message(message.chat.id, response, parse_mode='Markdown')
    else:
        # Ð¡Ñ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð²Ð¾Ð¿Ñ€Ð¾ÑÐ° Ñ‡ÐµÑ€ÐµÐ· RAG
        answer, source_docs = assistant.get_answer(user_query, user_id)

        # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸
        sources = [doc.metadata.get("file_name") for doc in source_docs if doc.metadata.get("file_name")]
        sources = list(set(sources))  # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹

        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð¼
        response_message = answer
        if sources:
            response_message += f"\n\nðŸ“š *Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸:* {', '.join(sources)}"

        # Ð Ð°Ð·Ð±Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¸ Ð½ÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚Ð¸
        if len(response_message) > 4000:
            chunks = [response_message[i:i + 4000] for i in range(0, len(response_message), 4000)]
            for chunk in chunks:
                bot.send_message(message.chat.id, chunk, parse_mode='Markdown')
        else:
            bot.send_message(message.chat.id, response_message, parse_mode='Markdown')

if __name__ == "__main__":
    logger.info("Starting Themis Telegram bot...")
    try:
        bot.polling(none_stop=True)
    except Exception as e:
        logger.error(f"Error in bot polling: {e}")